# ğŸ¦Œ Wildlife Detection & Tracking from Drone Footage - Production PoC

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Installation & Setup](#installation--setup)
- [Running the Application](#running-the-application)
- [Demo Guide](#demo-guide)
- [API Documentation](#api-documentation)
- [Deployment](#deployment)

---

## ğŸ¯ Overview

A **production-ready Proof of Concept (PoC)** to analyze **drone-captured images and videos** over forests/jungles with AI-powered wildlife detection and tracking capabilities.

### Key Capabilities:
- âœ… Detect animals in images and videos using **YOLO11** (latest state-of-the-art model)
- âœ… Identify animal **groups/herds** using spatial clustering
- âœ… Track animals across video frames in real-time
- âœ… Associate detections with **GPS coordinates** and metadata
- âœ… Modern **Next.js web interface** for demonstrations
- âœ… RESTful API for integration with other systems
- âœ… Optimized for **Apple Silicon (M4)** with Metal Performance Shaders (MPS)
- âœ… AWS-ready architecture for cloud deployment

### Demo Outputs:
- ğŸ“¸ Annotated images/videos with bounding boxes
- ğŸ¯ Species classification with confidence scores
- ğŸ¾ Group/herd identification
- ğŸ“ GPS-tagged detections
- ğŸ“Š Real-time tracking visualization
- ğŸ“„ JSON/CSV export for data analysis

---

## ğŸš€ Features

### Phase 1: Image Detection âœ…
- Upload single or multiple drone images
- Real-time animal detection with YOLO11
- Visual bounding boxes with species labels
- Confidence scoring
- Export results as JSON

### Phase 2: Grouping & Clustering âœ…
- Spatial clustering algorithm (DBSCAN/HDBSCAN)
- Automatic herd/group identification
- Group statistics (count, spread, density)

### Phase 3: Video Tracking âœ…
- Frame-by-frame processing
- Multi-object tracking (ByteTrack)
- Trajectory visualization
- Track persistence across occlusions

### Phase 4: Location Intelligence âœ…
- GPS metadata extraction (EXIF)
- Drone telemetry integration
- Geospatial mapping of detections
- Altitude and timestamp correlation

---

## ğŸ›  Tech Stack

### Backend
- **Python 3.11+**
- **FastAPI** - Modern async web framework
- **Ultralytics YOLO11** - Latest object detection model
- **PyTorch 2.x** - Deep learning framework (MPS backend for Mac)
- **OpenCV** - Computer vision operations
- **ByteTrack** - Multi-object tracking
- **Scikit-learn** - Clustering algorithms
- **Pillow & ExifRead** - Image processing and metadata

### Frontend
- **Next.js 14** - React framework with App Router
- **React 18** - UI library
- **TypeScript** - Type safety
- **Tailwind CSS** - Styling
- **Shadcn/ui** - UI components
- **React Query** - Data fetching
- **Axios** - HTTP client
- **React Dropzone** - File uploads
- **Leaflet** - Map visualization

### Infrastructure
- **Docker & Docker Compose** - Containerization
- **Nginx** - Reverse proxy (production)
- **AWS S3** - File storage (cloud deployment)
- **AWS ECS/Fargate** - Container orchestration (cloud deployment)

---

## ğŸ“ Project Structure

```
wildlife-drone-poc/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”‚   â”œâ”€â”€ config.py               # Configuration settings
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ detector.py         # YOLO11 detection
â”‚   â”‚   â”‚   â”œâ”€â”€ tracker.py          # ByteTrack tracking
â”‚   â”‚   â”‚   â””â”€â”€ grouping.py         # Clustering logic
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ image_service.py    # Image processing
â”‚   â”‚   â”‚   â”œâ”€â”€ video_service.py    # Video processing
â”‚   â”‚   â”‚   â””â”€â”€ metadata_service.py # GPS extraction
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py           # API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic models
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ file_handler.py
â”‚   â”‚       â””â”€â”€ visualization.py
â”‚   â”œâ”€â”€ weights/                     # YOLO11 model weights
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx            # Home page
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Root layout
â”‚   â”‚   â”‚   â”œâ”€â”€ upload/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ page.tsx        # Upload interface
â”‚   â”‚   â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ page.tsx        # Results visualization
â”‚   â”‚   â”‚   â””â”€â”€ api/                # API routes (if needed)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/                 # Shadcn components
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadZone.tsx      # Drag & drop upload
â”‚   â”‚   â”‚   â”œâ”€â”€ ResultsViewer.tsx   # Detection results
â”‚   â”‚   â”‚   â”œâ”€â”€ VideoPlayer.tsx     # Video with tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ MapViewer.tsx       # GPS visualization
â”‚   â”‚   â”‚   â””â”€â”€ StatsPanel.tsx      # Analytics dashboard
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts              # API client
â”‚   â”‚   â”‚   â””â”€â”€ utils.ts            # Utilities
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â””â”€â”€ index.ts            # TypeScript types
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tailwind.config.ts
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original uploads
â”‚   â”œâ”€â”€ frames/                     # Extracted video frames
â”‚   â”œâ”€â”€ processed/                  # Annotated outputs
â”‚   â””â”€â”€ results/                    # JSON/CSV exports
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # Multi-container setup
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_models.py          # Download YOLO11 weights
â”‚   â””â”€â”€ test_detection.py           # Quick test script
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_backend.py
â”‚   â””â”€â”€ test_frontend.py
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ PROJECT_SETUP_GUIDE.md          # This file
```

---

## ğŸ“‹ Prerequisites

### System Requirements
- **OS**: macOS (Apple Silicon M4 recommended) or Linux
- **RAM**: 16GB minimum, 48GB recommended
- **Python**: 3.11 or higher
- **Node.js**: 18.x or higher
- **Git**: Latest version

### Optional
- **Docker Desktop**: For containerized deployment
- **CUDA GPU**: For NVIDIA GPU acceleration (Linux/Windows)

---

## ğŸ’» Installation & Setup

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd wildlife-drone-poc
```

### Step 2: Backend Setup

#### 2.1 Create Python Virtual Environment

```bash
cd backend
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 2.2 Install Python Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 2.3 Download YOLO11 Model Weights

```bash
# Run the download script
python ../scripts/download_models.py

# Or manually download using Python
python -c "from ultralytics import YOLO; YOLO('yolo11n.pt')"
```

Available YOLO11 models (from fastest to most accurate):
- `yolo11n.pt` - Nano (fastest, ~6MB)
- `yolo11s.pt` - Small (~22MB)
- `yolo11m.pt` - Medium (~50MB)
- `yolo11l.pt` - Large (~100MB)
- `yolo11x.pt` - Extra Large (most accurate, ~200MB)

**Recommendation**: Start with `yolo11m.pt` for balanced performance.

#### 2.4 Configure Environment Variables

```bash
# Copy example environment file
cp ../.env.example .env

# Edit .env with your settings
nano .env
```

**Example `.env` file:**
```env
# Backend Configuration
PYTHON_ENV=development
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000

# Model Configuration
YOLO_MODEL_PATH=weights/yolo11m.pt
CONFIDENCE_THRESHOLD=0.25
IOU_THRESHOLD=0.45
MAX_DETECTIONS=300

# Storage Configuration
UPLOAD_DIR=../data/raw
PROCESSED_DIR=../data/processed
RESULTS_DIR=../data/results

# Tracking Configuration
TRACKER_TYPE=bytetrack
TRACK_BUFFER=30
MATCH_THRESHOLD=0.8

# Performance
DEVICE=mps  # Use 'mps' for Mac, 'cuda' for NVIDIA GPU, 'cpu' for CPU
BATCH_SIZE=1
NUM_WORKERS=4
```

### Step 3: Frontend Setup

#### 3.1 Install Node.js Dependencies

```bash
cd ../frontend
npm install
# Or use yarn/pnpm
# yarn install
# pnpm install
```

#### 3.2 Configure Frontend Environment

```bash
cp .env.example .env.local
nano .env.local
```

**Example `.env.local` file:**
```env
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_APP_NAME=Wildlife Drone Tracker
NEXT_PUBLIC_MAX_FILE_SIZE=100  # MB
NEXT_PUBLIC_SUPPORTED_FORMATS=jpg,jpeg,png,mp4,mov,avi
```

### Step 4: Create Required Directories

```bash
cd ..
mkdir -p data/{raw,frames,processed,results}
mkdir -p backend/weights
```

---

## ğŸ¬ Running the Application

### Option 1: Manual Run (Development)

#### Terminal 1: Start Backend

```bash
cd backend
source venv/bin/activate
python app/main.py

# Or use uvicorn directly
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Backend will be available at: **http://localhost:8000**
API documentation: **http://localhost:8000/docs**

#### Terminal 2: Start Frontend

```bash
cd frontend
npm run dev
```

Frontend will be available at: **http://localhost:3000**

### Option 2: Docker Compose (Production-like)

```bash
# Build and start all services
docker-compose up --build

# Or run in detached mode
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Services:
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

---

## ğŸ¨ Demo Guide for Customer Presentation

### Preparation Checklist

1. **Download Sample Drone Footage**
   ```bash
   # Place sample files in data/raw/
   # Recommended: 2-3 images and 1 short video (30-60 seconds)
   ```

2. **Test the System**
   ```bash
   cd scripts
   python test_detection.py
   ```

3. **Prepare Demo Script** (see below)

### Demo Flow (15-20 minutes)

#### Part 1: System Overview (3 mins)
- Show the landing page
- Explain the use case: wildlife monitoring from drones
- Highlight key features

#### Part 2: Image Detection (5 mins)
1. Navigate to Upload page
2. Upload a drone image with visible animals
3. Click "Analyze"
4. Show real-time detection results:
   - Bounding boxes around animals
   - Species classification
   - Confidence scores
5. Download JSON results
6. Explain grouping/clustering visualization

#### Part 3: Video Tracking (7 mins)
1. Upload a short video clip
2. Show processing progress
3. Display tracking results:
   - Each animal assigned unique ID
   - Tracking paths/trajectories
   - Frame-by-frame playback
4. Export tracking data

#### Part 4: GPS & Metadata (3 mins)
1. Upload image with GPS metadata
2. Show detection mapped on geographical view
3. Display metadata panel:
   - Coordinates
   - Altitude
   - Timestamp
4. Export georeferenced data

#### Part 5: Q&A and Technical Deep Dive (5 mins)
- Show API documentation
- Discuss accuracy metrics
- Explain scalability (AWS deployment)
- Answer customer questions

### Demo Tips
âœ… Use high-quality drone footage with clear animal visibility
âœ… Pre-load files to avoid upload delays
âœ… Have backup samples ready
âœ… Practice the flow beforehand
âœ… Keep technical jargon minimal unless asked
âœ… Focus on business value and ROI

---

## ğŸ“š API Documentation

### Core Endpoints

#### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "model": "yolo11m",
  "device": "mps",
  "version": "1.0.0"
}
```

#### 2. Upload & Detect Image
```http
POST /api/detect/image
Content-Type: multipart/form-data

file: <image_file>
confidence: 0.25 (optional)
```

**Response:**
```json
{
  "success": true,
  "filename": "drone_image.jpg",
  "detections": [
    {
      "id": 1,
      "class": "deer",
      "confidence": 0.87,
      "bbox": [150, 200, 300, 400],
      "group_id": 1
    }
  ],
  "groups": [
    {
      "group_id": 1,
      "count": 3,
      "center": [225, 300]
    }
  ],
  "metadata": {
    "gps": {"lat": 37.7749, "lon": -122.4194},
    "altitude": 120.5,
    "timestamp": "2025-12-14T10:30:00Z"
  },
  "annotated_image_url": "/results/annotated_drone_image.jpg"
}
```

#### 3. Upload & Track Video
```http
POST /api/detect/video
Content-Type: multipart/form-data

file: <video_file>
confidence: 0.25 (optional)
fps: 5 (optional, frames to process per second)
```

**Response:**
```json
{
  "success": true,
  "filename": "drone_video.mp4",
  "total_frames": 300,
  "processed_frames": 150,
  "tracks": [
    {
      "track_id": 1,
      "class": "elephant",
      "first_frame": 10,
      "last_frame": 145,
      "trajectory": [[x1, y1], [x2, y2], ...]
    }
  ],
  "annotated_video_url": "/results/tracked_drone_video.mp4"
}
```

#### 4. Export Results
```http
GET /api/export/{job_id}?format=json|csv
```

#### 5. List Results
```http
GET /api/results
```

For complete API documentation, visit **http://localhost:8000/docs** when the backend is running.

---

## ğŸŒ Deployment

### AWS Deployment Architecture

#### Infrastructure Components:
- **ECS Fargate**: Container orchestration
- **S3**: File storage for uploads and results
- **CloudFront**: CDN for frontend
- **ALB**: Load balancing
- **RDS/DynamoDB**: Metadata storage (optional)
- **CloudWatch**: Logging and monitoring

#### Deployment Steps:

1. **Build Docker Images**
   ```bash
   docker build -t wildlife-backend:latest ./backend
   docker build -t wildlife-frontend:latest ./frontend
   ```

2. **Push to ECR**
   ```bash
   aws ecr create-repository --repository-name wildlife-backend
   aws ecr create-repository --repository-name wildlife-frontend
   
   # Tag and push
   docker tag wildlife-backend:latest <account-id>.dkr.ecr.<region>.amazonaws.com/wildlife-backend:latest
   docker push <account-id>.dkr.ecr.<region>.amazonaws.com/wildlife-backend:latest
   ```

3. **Deploy with Terraform/CloudFormation** (create IaC scripts)

4. **Configure Environment Variables** in ECS Task Definitions

---

## ğŸ“Š Performance Optimization

### Mac M4 Optimization:
- âœ… Metal Performance Shaders (MPS) enabled
- âœ… Batch processing for multiple images
- âœ… Async video processing
- âœ… Optimized model size (yolo11m recommended)

### Expected Performance:
- **Image Detection**: ~200-300ms per image
- **Video Tracking**: ~5-10 FPS processing speed
- **Memory Usage**: ~4-6GB during inference

---

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest tests/

# Frontend tests
cd frontend
npm run test
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit changes
4. Push to the branch
5. Create a Pull Request

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ†˜ Troubleshooting

### Issue: YOLO model not found
**Solution**: Run `python scripts/download_models.py`

### Issue: MPS not available on Mac
**Solution**: Update PyTorch to latest version, set `DEVICE=cpu` in `.env`

### Issue: Frontend can't connect to backend
**Solution**: Check CORS settings, ensure backend is running on port 8000

### Issue: Out of memory during video processing
**Solution**: Reduce batch size, process fewer frames per second

---

## ğŸ“ Support

For questions or issues:
- Create a GitHub Issue
- Email: support@example.com

---

**Built with â¤ï¸ using YOLO11, Next.js, and FastAPI**

